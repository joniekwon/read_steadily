# Read_steadily
꾸준히 읽기

## blog
- optimizer(AdamW, AdamRW...) [[LINK](https://hiddenbeginner.github.io/deeplearning/paperreview/2019/12/29/paper_review_AdamW.html)]

## Paper
- Transformers in Vision: A Survey [[LINK](https://arxiv.org/abs/2101.01169)]
- Transformer [[LINK](https://paperswithcode.com/method/transformer)]
- Attention Is All You Need [[LINK](https://paperswithcode.com/paper/attention-is-all-you-need)]
- Self-Supervised Learning [[LINK](https://paperswithcode.com/task/self-supervised-learning)]
- Adam[[LINK](https://arxiv.org/abs/1412.6980)]
- Adadelta[[LINK](https://arxiv.org/abs/1212.5701)]
